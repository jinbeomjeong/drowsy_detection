{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-27T01:09:48.465332700Z",
     "start_time": "2024-02-27T01:09:41.324888800Z"
    }
   },
   "outputs": [],
   "source": [
    "import time, cv2, torch, os\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from models.experimental import attempt_load\n",
    "from utils.augmentations import letterbox\n",
    "from utils.general import check_img_size, non_max_suppression, scale_coords \n",
    "from utils.accessory_lib import system_info\n",
    "from PIPNet.networks import Pip_resnet101\n",
    "from PIPNet.functions import forward_pip, get_meanface\n",
    "from scipy.spatial import distance\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "bus_img_dir = \"D:\\\\drowsy_dataset\\\\image\\\\bus\\\\\"\n",
    "passenger_img_dir = \"D:\\\\drowsy_dataset\\\\image\\\\passenger\\\\\"\n",
    "taxi_img_dir = \"D:\\\\drowsy_dataset\\\\image\\\\taxi\\\\\"\n",
    "truck_img_dir = \"D:\\\\drowsy_dataset\\\\image\\\\truck\\\\\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T01:09:50.808932800Z",
     "start_time": "2024-02-27T01:09:50.784120300Z"
    }
   },
   "id": "70dfe57bdbfb3717"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "8067"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bus_img_path_list = []\n",
    "\n",
    "for sub_dir_name in os.listdir(bus_img_dir):\n",
    "    for img_name in os.listdir(bus_img_dir+sub_dir_name):\n",
    "        bus_img_path_list.append(bus_img_dir+sub_dir_name+os.sep+img_name)\n",
    "\n",
    "bus_img_path_list.sort()\n",
    "len(bus_img_path_list)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T01:09:51.987507800Z",
     "start_time": "2024-02-27T01:09:51.885447400Z"
    }
   },
   "id": "699daa29b9f55494"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "8903"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passenger_img_path_list = []\n",
    "\n",
    "for sub_dir_name in os.listdir(passenger_img_dir):\n",
    "    for img_name in os.listdir(passenger_img_dir+sub_dir_name):\n",
    "        passenger_img_path_list.append(passenger_img_dir+sub_dir_name+os.sep+img_name)\n",
    "\n",
    "passenger_img_path_list.sort()\n",
    "len(passenger_img_path_list)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T01:09:53.225335Z",
     "start_time": "2024-02-27T01:09:53.132408500Z"
    }
   },
   "id": "4b5326108ea3509",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "3200"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_img_path_list = []\n",
    "\n",
    "for sub_dir_name in os.listdir(taxi_img_dir):\n",
    "    for img_name in os.listdir(taxi_img_dir+sub_dir_name):\n",
    "        taxi_img_path_list.append(taxi_img_dir+sub_dir_name+os.sep+img_name)\n",
    "\n",
    "taxi_img_path_list.sort()\n",
    "len(taxi_img_path_list)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T01:09:54.457726600Z",
     "start_time": "2024-02-27T01:09:54.408045200Z"
    }
   },
   "id": "ede4d4d2ced1433e",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "3344"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truck_img_path_list = []\n",
    "\n",
    "for sub_dir_name in os.listdir(truck_img_dir):\n",
    "    for img_name in os.listdir(truck_img_dir+sub_dir_name):\n",
    "        truck_img_path_list.append(truck_img_dir+sub_dir_name+os.sep+img_name)\n",
    "\n",
    "truck_img_path_list.sort()\n",
    "len(truck_img_path_list)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T01:09:55.348787400Z",
     "start_time": "2024-02-27T01:09:55.298414Z"
    }
   },
   "id": "4d5e85b3a3a2512b",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ----- Environment Status -----\n",
      "Operating System Type: ('64bit', 'WindowsPE')\n",
      "Python Version: 3.8.10 (tags/v3.8.10:3d8993a, May  3 2021, 11:48:03) [MSC v.1928 64 bit (AMD64)]\n",
      "Pytorch Version: 1.13.1+cu116\n",
      "Open Computer Vision Version: 4.8.0\n",
      "GPU Available: True\n",
      "GPU Device Name: NVIDIA GeForce RTX 4090\n",
      "Number of GPU: 1\n",
      "Tensor Core TF32 Enable: False \n",
      "\n",
      "[1/3] Device Initialized 4.25sec\n"
     ]
    }
   ],
   "source": [
    "net_stride = 32\n",
    "num_nb = 10\n",
    "data_name = 'data_300W'\n",
    "experiment_name = 'pip_32_16_60_r101_l2_l1_10_1_nb10'\n",
    "num_lms = 68\n",
    "face_landmark_input_size = 240\n",
    "det_box_scale = 1.2\n",
    "eye_det = 0.12\n",
    "\n",
    "img_size = 640\n",
    "CONF_THRES = 0.2\n",
    "IOU_THRES = 0.3\n",
    "half = True\n",
    "cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "\n",
    "prev_time = time.time()\n",
    "start_time = time.time()\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "meanface_indices, reverse_index1, reverse_index2, max_len = get_meanface(os.path.join('PIPNet', 'data', data_name,\n",
    "                                                                                      'meanface.txt'), num_nb)\n",
    "\n",
    "resnet101 = models.resnet101(weights='ResNet101_Weights.DEFAULT')\n",
    "face_landmark_net = Pip_resnet101(resnet101, num_nb=num_nb, num_lms=num_lms, input_size=face_landmark_input_size,\n",
    "                                  net_stride=net_stride)\n",
    "face_landmark_net = face_landmark_net.to(device)\n",
    "\n",
    "face_landmark_weight_file_path = os.path.join('PIPNet', 'snapshots', data_name, experiment_name, f'epoch{60-1}.pth')\n",
    "face_landmark_net.load_state_dict(torch.load(f=face_landmark_weight_file_path, map_location=device))\n",
    "face_landmark_net.eval()\n",
    "\n",
    "face_landmark_normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "preprocess = transforms.Compose([transforms.ToPILImage(),\n",
    "                                 transforms.Resize((face_landmark_input_size, face_landmark_input_size)),\n",
    "                                 transforms.ToTensor(), face_landmark_normalize])\n",
    "\n",
    "face_detector_weight_file_path = os.path.join('.', 'weights', 'face_detection_yolov5s.pt')\n",
    "face_det_normalize_tensor = torch.tensor(255.0).to(device)\n",
    "\n",
    "system_info()\n",
    "\n",
    "# Initialize\n",
    "print(f'[1/3] Device Initialized {time.time() - prev_time:.2f}sec')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T01:10:00.780082300Z",
     "start_time": "2024-02-27T01:09:56.524527300Z"
    }
   },
   "id": "9f2be4be5900fef0"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "newYOLOv5s summary: 224 layers, 7053910 parameters, 0 gradients\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/3] Yolov5 Detector Model Loaded 3.47sec\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "prev_time = time.time()\n",
    "model = attempt_load(face_detector_weight_file_path, device)  # load FP32 model\n",
    "model.eval()\n",
    "stride = int(model.stride.max())  # model stride\n",
    "img_size_chk = check_img_size(img_size, s=stride)  # check img_size\n",
    "\n",
    "if half:\n",
    "    model.half()  # to FP16\n",
    "\n",
    "# Get names and colors\n",
    "names = model.module.names if hasattr(model, 'module') else model.names\n",
    "\n",
    "# Run inference\n",
    "model(torch.zeros(1, 3, img_size_chk, img_size_chk).to(device).type_as(next(model.parameters())))  # run once\n",
    "print(f'[2/3] Yolov5 Detector Model Loaded {time.time() - prev_time:.2f}sec')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T01:57:22.506041300Z",
     "start_time": "2024-02-26T01:57:19.029827700Z"
    }
   },
   "id": "aad9e78608ed293e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "predict_data_columns = ['file_name', 'face_pt1_x_pos', 'face_pt1_y_pos', 'face_pt2_x_pos', 'face_pt2_y_pos', \n",
    "                        'left_eye_pt1_x_pos', 'left_eye_pt1_y_pos', 'left_eye_pt2_x_pos', 'left_eye_pt2_y_pos',\n",
    "                        'right_eye_pt1_x_pos', 'right_eye_pt1_y_pos', 'right_eye_pt2_x_pos', 'right_eye_pt2_y_pos',\n",
    "                        'left_eye_horizontal', 'left_eye_vertical', 'right_eye_horizontal', 'right_eye_vertical',\n",
    "                        'face_detection', 'left_eye_close', 'right_eye_close', 'result_close', 'face_confidence']\n",
    "\n",
    "logging_data = pd.DataFrame()\n",
    "logging_header = pd.DataFrame(columns=predict_data_columns)\n",
    "\n",
    "start_time_str = time.strftime('%Y%m%d-%H%M%S', time.localtime(time.time()))\n",
    "logging_file_name = 'predict' + '_' + start_time_str\n",
    "logging_file_path = 'analysis_data' + os.sep + logging_file_name + '.csv'\n",
    "logging_header.to_csv(logging_file_path, mode='a', header=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T01:57:22.533473200Z",
     "start_time": "2024-02-26T01:57:22.511041300Z"
    }
   },
   "id": "ae09c0cdd748d9f7",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "face_bbox = np.zeros(4, dtype=np.float64)\n",
    "left_eye_bbox = np.zeros(4, dtype=np.float64)\n",
    "right_eye_bbox = np.zeros(4, dtype=np.float64)\n",
    "eye_shape_arr = np.zeros(4, dtype=np.float64)  # left eye horizontal, left eye vertical, right eye horizontal, right eye vertical\n",
    "detection_status = np.zeros(5, dtype=np.int32)  # face detection, left eye detection, right eye detection, result detection, face confidence"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T01:57:22.535479500Z",
     "start_time": "2024-02-26T01:57:22.525005900Z"
    }
   },
   "id": "194e70908a8b7b12",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predicting for eye detection: 100%|██████████| 3344/3344 [02:51<00:00, 19.47it/s]\n"
     ]
    }
   ],
   "source": [
    "with (torch.no_grad()):\n",
    "    for img_name in tqdm(truck_img_path_list, desc='predicting for eye detection'):\n",
    "        face_bbox.fill(0)\n",
    "        left_eye_bbox.fill(0)\n",
    "        right_eye_bbox.fill(0)\n",
    "        eye_shape_arr.fill(0)\n",
    "        detection_status.fill(0)\n",
    "                \n",
    "        _, sample_file_name = os.path.split(img_name)\n",
    "        img0 = cv2.imread(img_name) \n",
    "        img = letterbox(img0, img_size_chk, stride=stride)[0] # Padded resize\n",
    "\n",
    "        # Convert\n",
    "        img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
    "        img = np.ascontiguousarray(img)\n",
    "\n",
    "        img = torch.from_numpy(img)\n",
    "        img = img.half() if half else img.float()  # uint8 to fp16/32\n",
    "        img = torch.divide(img.to(device), face_det_normalize_tensor)\n",
    "        img = img.unsqueeze(0)\n",
    "        \n",
    "        pred = model(img, augment=False)[0]  # Inference\n",
    " \n",
    "        detected_face = non_max_suppression(pred, CONF_THRES, IOU_THRES, classes=None, agnostic=False)[0]  # Apply NMS\n",
    "\n",
    "        # Process detections\n",
    "        if len(detected_face):\n",
    "            detected_face[:, :4] = scale_coords(img.shape[2:], detected_face[:, :4], img0.shape).round()  # Rescale boxes from img_size to img0 size\n",
    "            detection_status[0] = 1\n",
    "            \n",
    "            for *xyxy, conf, cls in reversed(detected_face):  # Write results\n",
    "                face_det_conf = float(conf)\n",
    "                detection_status[4] = int(face_det_conf*1000)\n",
    "                det_xmin = int(xyxy[0])\n",
    "                det_ymin = int(xyxy[1])\n",
    "                det_xmax = int(xyxy[2])\n",
    "                det_ymax = int(xyxy[3])\n",
    "                \n",
    "                det_width = det_xmax - det_xmin\n",
    "                det_height = det_ymax - det_ymin\n",
    "                \n",
    "                for i, pos in enumerate(xyxy):\n",
    "                    face_bbox[i] = float(pos)\n",
    "                \n",
    "                det_xmin -= int(det_width * (det_box_scale - 1) / 2)\n",
    "                det_ymin -= int(det_height * (det_box_scale - 1) / 2)\n",
    "                det_xmax += int(det_width * (det_box_scale - 1) / 2)\n",
    "                det_ymax += int(det_height * (det_box_scale - 1) / 2)\n",
    "\n",
    "                det_xmin = max(det_xmin, 0)\n",
    "                det_ymin = max(det_ymin, 0)\n",
    "                det_xmax = min(det_xmax, img0.shape[0] - 1)\n",
    "                det_ymax = min(det_ymax, img0.shape[1] - 1)\n",
    "\n",
    "                det_width = det_xmax - det_xmin + 1\n",
    "                det_height = det_ymax - det_ymin + 1\n",
    "                det_crop = img0[det_ymin:det_ymax, det_xmin:det_xmax, :]\n",
    "                \n",
    "                if det_crop.shape[0] > 0 and det_crop.shape[1] > 0:\n",
    "                    det_crop = cv2.resize(src=det_crop, dsize=(face_landmark_input_size, face_landmark_input_size),\n",
    "                                          interpolation=cv2.INTER_AREA)\n",
    "                    cv2.rectangle(img0, (det_xmin, det_ymin), (det_xmax, det_ymax), (0, 0, 255), 1)\n",
    "\n",
    "                    inputs = preprocess(det_crop).to(device)\n",
    "                    inputs = inputs.unsqueeze(0)\n",
    "                    lms_pred_x, lms_pred_y, lms_pred_nb_x, lms_pred_nb_y, outputs_cls, max_cls = forward_pip(face_landmark_net,\n",
    "                                                                                                             inputs, preprocess,\n",
    "                                                                                                             face_landmark_input_size,\n",
    "                                                                                                             net_stride, num_nb)\n",
    "\n",
    "                    lms_pred = torch.cat((lms_pred_x, lms_pred_y), dim=1).flatten()\n",
    "                    tmp_nb_x = lms_pred_nb_x[reverse_index1, reverse_index2].view(num_lms, max_len)\n",
    "                    tmp_nb_y = lms_pred_nb_y[reverse_index1, reverse_index2].view(num_lms, max_len)\n",
    "                    tmp_x = torch.mean(torch.cat((lms_pred_x, tmp_nb_x), dim=1), dim=1).view(-1, 1)\n",
    "                    tmp_y = torch.mean(torch.cat((lms_pred_y, tmp_nb_y), dim=1), dim=1).view(-1, 1)\n",
    "\n",
    "                    lms_pred_merge = torch.cat((tmp_x, tmp_y), dim=1).flatten()\n",
    "                    lms_pred = lms_pred.cpu().numpy()\n",
    "                    lms_pred_merge = lms_pred_merge.cpu().numpy()\n",
    "\n",
    "                    eye_x = (lms_pred_merge[36 * 2:48 * 2:2] * det_width).astype(np.int32) + det_xmin\n",
    "                    eye_y = (lms_pred_merge[(36 * 2) + 1:(48 * 2) + 1:2] * det_height).astype(np.int32) + det_ymin\n",
    "\n",
    "                    left_eye_horizontal_dist = distance.euclidean((eye_x[0], eye_y[0]), (eye_x[3], eye_y[3]))\n",
    "                    left_eye_vertical_dist = min(distance.euclidean((eye_x[1], eye_y[1]), (eye_x[5], eye_y[5])),\n",
    "                                                 distance.euclidean((eye_x[2], eye_y[2]), (eye_x[4], eye_y[4])))\n",
    "\n",
    "                    right_eye_horizontal_dist = distance.euclidean((eye_x[6], eye_y[6]), (eye_x[9], eye_y[9]))\n",
    "                    right_eye_vertical_dist = min(distance.euclidean((eye_x[11], eye_y[11]), (eye_x[7], eye_y[7])),\n",
    "                                                  distance.euclidean((eye_x[10], eye_y[10]), (eye_x[8], eye_y[8])))\n",
    "\n",
    "                    left_eye_ratio = left_eye_vertical_dist / left_eye_horizontal_dist\n",
    "                    right_eye_ratio = right_eye_vertical_dist / right_eye_horizontal_dist\n",
    "                    \n",
    "                    eye_shape_arr[0] = left_eye_horizontal_dist\n",
    "                    eye_shape_arr[1] = left_eye_vertical_dist\n",
    "                    eye_shape_arr[2] = right_eye_horizontal_dist\n",
    "                    eye_shape_arr[3] = right_eye_vertical_dist\n",
    "                    \n",
    "                    left_eye_det_result = left_eye_ratio < eye_det\n",
    "                    right_eye_det_result = right_eye_ratio < eye_det\n",
    "                    \n",
    "                    drowsy_det = left_eye_det_result and right_eye_det_result\n",
    "                    \n",
    "                    detection_status[1] = int(left_eye_det_result)\n",
    "                    detection_status[2] = int(right_eye_det_result)\n",
    "                    detection_status[3] = int(drowsy_det)\n",
    "                    \n",
    "                    left_eye_bbox[0] = np.min(eye_x[0:5])\n",
    "                    left_eye_bbox[1] = np.min(eye_y[0:5])\n",
    "                    left_eye_bbox[2] = np.max(eye_x[0:5])\n",
    "                    left_eye_bbox[3] = np.max(eye_y[0:5])\n",
    "\n",
    "                    right_eye_bbox[0] = np.min(eye_x[6:11])\n",
    "                    right_eye_bbox[1] = np.min(eye_y[6:11])\n",
    "                    right_eye_bbox[2] = np.max(eye_x[6:11])\n",
    "                    right_eye_bbox[3] = np.max(eye_y[6:11])\n",
    "                    \n",
    "        result_data = np.concatenate([np.array([sample_file_name]), face_bbox, left_eye_bbox, right_eye_bbox, eye_shape_arr, detection_status])\n",
    "        logging_data = pd.DataFrame(data=[result_data], columns=predict_data_columns)\n",
    "        logging_data.to_csv(logging_file_path, mode='a', header=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T02:00:14.356314300Z",
     "start_time": "2024-02-26T01:57:22.549481Z"
    }
   },
   "id": "23dcb4eaaf86b78f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3b265224c8b86110"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
