{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-17T10:46:36.989408Z",
     "start_time": "2024-03-17T10:46:36.985407Z"
    }
   },
   "outputs": [],
   "source": [
    "import time, cv2, torch, os\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from models.experimental import attempt_load\n",
    "from utils.augmentations import letterbox\n",
    "from utils.general import check_img_size, non_max_suppression, scale_coords\n",
    "from utils.accessory_lib import system_info\n",
    "from PIPNet.networks import Pip_resnet101\n",
    "from PIPNet.functions import forward_pip, get_meanface\n",
    "from scipy.spatial import distance\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "net_stride = 32\n",
    "num_nb = 10\n",
    "data_name = 'data_300W'\n",
    "experiment_name = 'pip_32_16_60_r101_l2_l1_10_1_nb10'\n",
    "num_lms = 68\n",
    "face_landmark_input_size = 240\n",
    "det_box_scale = 1.2\n",
    "eye_det = 0.11\n",
    "\n",
    "img_size = 640\n",
    "CONF_THRES = 0.4\n",
    "IOU_THRES = 0.45\n",
    "cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "\n",
    "elapsed_time: float = 0.0\n",
    "fps: float = 0.0\n",
    "ref_frame: int = 0\n",
    "det_frame: int = 0\n",
    "prev_time = time.time()\n",
    "start_time = time.time()\n",
    "t0 = time.time()\n",
    "\n",
    "left_eye_det_prv = False\n",
    "right_eye_det_prv = False\n",
    "left_eye_det_result = False\n",
    "right_eye_det_result = False\n",
    "left_eye_ratio: float = 0.0\n",
    "right_eye_ratio: float = 0.0\n",
    "drowsy_det = False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T10:46:36.995408Z",
     "start_time": "2024-03-17T10:46:36.990407Z"
    }
   },
   "id": "7acaae939fddf8fd",
   "execution_count": 172
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "400"
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_dir = \"C:\\\\workspace\\\\drowsy_image\\\\bus\\\\R_217_40_M\"\n",
    "\n",
    "img_path_list = []\n",
    "\n",
    "for img_name in os.listdir(img_dir):\n",
    "    img_path_list.append(img_dir+os.sep+img_name)\n",
    "\n",
    "img_path_list.sort()\n",
    "len(img_path_list)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T10:46:37.001414Z",
     "start_time": "2024-03-17T10:46:36.996408Z"
    }
   },
   "id": "be340de9fd6bbae3",
   "execution_count": 173
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "newYOLOv5s summary: 224 layers, 7053910 parameters, 0 gradients\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ----- Environment Status -----\n",
      "Operating System Type: ('64bit', 'WindowsPE')\n",
      "Python Version: 3.8.10 (tags/v3.8.10:3d8993a, May  3 2021, 11:48:03) [MSC v.1928 64 bit (AMD64)]\n",
      "Pytorch Version: 1.13.1+cu116\n",
      "Open Computer Vision Version: 4.9.0\n",
      "GPU Available: True\n",
      "GPU Device Name: NVIDIA GeForce RTX 3080 Ti Laptop GPU\n",
      "Number of GPU: 1\n",
      "Tensor Core TF32 Enable: False \n",
      "\n",
      "[1/3] Device Initialized 1.02sec\n",
      "[2/3] Yolov5 Detector Model Loaded 0.11sec\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "meanface_indices, reverse_index1, reverse_index2, max_len = get_meanface(os.path.join('PIPNet', 'data', data_name,\n",
    "                                                                                      'meanface.txt'), num_nb)\n",
    "\n",
    "resnet101 = models.resnet101(weights='ResNet101_Weights.DEFAULT')\n",
    "face_landmark_net = Pip_resnet101(resnet101, num_nb=num_nb, num_lms=num_lms, input_size=face_landmark_input_size,\n",
    "                                  net_stride=net_stride)\n",
    "\n",
    "face_landmark_net = face_landmark_net.to(device)\n",
    "\n",
    "face_landmark_weight_file_path = os.path.join('PIPNet', 'snapshots', data_name, experiment_name, f'epoch{60-1}.pth')\n",
    "face_landmark_net.load_state_dict(torch.load(f=face_landmark_weight_file_path, map_location=device))\n",
    "face_landmark_net.eval()\n",
    "\n",
    "face_landmark_normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "preprocess = transforms.Compose([transforms.ToPILImage(),\n",
    "                                 transforms.Resize((face_landmark_input_size, face_landmark_input_size)),\n",
    "                                 transforms.ToTensor(), face_landmark_normalize])\n",
    "\n",
    "face_detector_weight_file_path = os.path.join('.', 'weights', 'face_detection_yolov5s.pt')\n",
    "face_det_normalize_tensor = torch.tensor(255.0).to(device)\n",
    "\n",
    "system_info()\n",
    "\n",
    "# Initialize\n",
    "print(f'[1/3] Device Initialized {time.time() - prev_time:.2f}sec')\n",
    "prev_time = time.time()\n",
    "\n",
    "# Load model\n",
    "model = attempt_load(face_detector_weight_file_path, device)  # load FP32 model\n",
    "model.eval()\n",
    "stride = int(model.stride.max())  # model stride\n",
    "img_size_chk = check_img_size(img_size, s=stride)  # check img_size\n",
    "\n",
    "# Get names and colors\n",
    "names = model.module.names if hasattr(model, 'module') else model.names\n",
    "\n",
    "# Run inference\n",
    "model(torch.zeros(1, 3, img_size_chk, img_size_chk).to(device).type_as(next(model.parameters())))  # run once\n",
    "print(f'[2/3] Yolov5 Detector Model Loaded {time.time() - prev_time:.2f}sec')\n",
    "prev_time = time.time()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T10:46:38.135485Z",
     "start_time": "2024-03-17T10:46:37.002408Z"
    }
   },
   "id": "c71f35aab84042ff",
   "execution_count": 174
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "img = cv2.imread(img_path_list[0])\n",
    "fps = 5\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MP4v')\n",
    "writer = cv2.VideoWriter('result_video.mp4', fourcc, fps, (img.shape[1], img.shape[0]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T10:46:38.152931Z",
     "start_time": "2024-03-17T10:46:38.136552Z"
    }
   },
   "id": "9447ede67f50e8c0",
   "execution_count": 175
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:06<00:00, 32.03it/s]\n"
     ]
    }
   ],
   "source": [
    "for img_name in tqdm(img_path_list[0:200]):\n",
    "    img0 = cv2.imread(img_name)\n",
    "    img = letterbox(img0, img_size_chk, stride=stride)[0]  # Padded resize\n",
    "\n",
    "    # Convert\n",
    "    img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
    "    img = np.ascontiguousarray(img)\n",
    "\n",
    "    img = torch.from_numpy(img)\n",
    "    img = torch.divide(img.to(device), face_det_normalize_tensor)\n",
    "    img = img.unsqueeze(0)\n",
    "\n",
    "    # Inference\n",
    "    pred = model(img, augment=False)[0]\n",
    "\n",
    "    # Apply NMS\n",
    "    detected_face = non_max_suppression(pred, CONF_THRES, IOU_THRES, classes=None, agnostic=False)[0]\n",
    "\n",
    "    # Process detections\n",
    "    if len(detected_face):\n",
    "        # Rescale boxes from img_size to img0 size\n",
    "        detected_face[:, :4] = scale_coords(img.shape[2:], detected_face[:, :4], img0.shape).round()\n",
    "\n",
    "        # Write results\n",
    "        for *xyxy, conf, cls in reversed(detected_face):\n",
    "            det_xmin = int(xyxy[0])\n",
    "            det_ymin = int(xyxy[1])\n",
    "            det_xmax = int(xyxy[2])\n",
    "            det_ymax = int(xyxy[3])\n",
    "            det_width = det_xmax - det_xmin\n",
    "            det_height = det_ymax - det_ymin\n",
    "\n",
    "            det_xmin -= int(det_width * (det_box_scale - 1) / 2)\n",
    "            det_ymin -= int(det_height * (det_box_scale - 1) / 2)\n",
    "            det_xmax += int(det_width * (det_box_scale - 1) / 2)\n",
    "            det_ymax += int(det_height * (det_box_scale - 1) / 2)\n",
    "\n",
    "            det_xmin = max(det_xmin, 0)\n",
    "            det_ymin = max(det_ymin, 0) \n",
    "            det_xmax = min(det_xmax, img0.shape[1] - 1)\n",
    "            det_ymax = min(det_ymax, img0.shape[0] - 1)\n",
    "            \n",
    "            det_width = det_xmax - det_xmin + 1\n",
    "            det_height = det_ymax - det_ymin + 1\n",
    "            det_crop = img0[det_ymin:det_ymax, det_xmin:det_xmax, :]\n",
    "            det_crop = cv2.resize(src=det_crop, dsize=(face_landmark_input_size, face_landmark_input_size),\n",
    "                                  interpolation=cv2.INTER_AREA)\n",
    "            cv2.rectangle(img0, (det_xmin, det_ymin), (det_xmax, det_ymax), (0, 0, 255), 1)\n",
    "\n",
    "            inputs = preprocess(det_crop).to(device)\n",
    "            inputs = inputs.unsqueeze(0)\n",
    "            lms_pred_x, lms_pred_y, lms_pred_nb_x, lms_pred_nb_y, outputs_cls, max_cls = forward_pip(face_landmark_net,\n",
    "                                                                                                     inputs, preprocess,\n",
    "                                                                                                     face_landmark_input_size,\n",
    "                                                                                                     net_stride, num_nb)\n",
    "\n",
    "            # lms_pred = torch.cat((lms_pred_x, lms_pred_y), dim=1).flatten()\n",
    "            tmp_nb_x = lms_pred_nb_x[reverse_index1, reverse_index2].view(num_lms, max_len)\n",
    "            tmp_nb_y = lms_pred_nb_y[reverse_index1, reverse_index2].view(num_lms, max_len)\n",
    "            tmp_x = torch.mean(torch.cat((lms_pred_x, tmp_nb_x), dim=1), dim=1).view(-1, 1)\n",
    "            tmp_y = torch.mean(torch.cat((lms_pred_y, tmp_nb_y), dim=1), dim=1).view(-1, 1)\n",
    "\n",
    "            lms_pred_merge = torch.cat((tmp_x, tmp_y), dim=1).flatten()\n",
    "            # lms_pred = lms_pred.cpu().numpy()\n",
    "            lms_pred_merge = lms_pred_merge.cpu().numpy()\n",
    "\n",
    "            eye_x = (lms_pred_merge[36 * 2:48 * 2:2] * det_width).astype(np.int32) + det_xmin\n",
    "            eye_y = (lms_pred_merge[(36 * 2) + 1:(48 * 2) + 1:2] * det_height).astype(np.int32) + det_ymin\n",
    "\n",
    "            left_eye_horizontal_dist = distance.euclidean((eye_x[0], eye_y[0]), (eye_x[3], eye_y[3]))\n",
    "            left_eye_vertical_dist = min(distance.euclidean((eye_x[1], eye_y[1]), (eye_x[5], eye_y[5])),\n",
    "                                         distance.euclidean((eye_x[2], eye_y[2]), (eye_x[4], eye_y[4])))\n",
    "\n",
    "            right_eye_horizontal_dist = distance.euclidean((eye_x[6], eye_y[6]), (eye_x[9], eye_y[9]))\n",
    "            right_eye_vertical_dist = min(distance.euclidean((eye_x[11], eye_y[11]), (eye_x[7], eye_y[7])),\n",
    "                                          distance.euclidean((eye_x[10], eye_y[10]), (eye_x[8], eye_y[8])))\n",
    "\n",
    "            left_eye_ratio = left_eye_vertical_dist / left_eye_horizontal_dist\n",
    "            right_eye_ratio = right_eye_vertical_dist / right_eye_horizontal_dist\n",
    "\n",
    "            left_eye_det_result = 0.01 < left_eye_ratio < eye_det\n",
    "            right_eye_det_result = 0.01 < right_eye_ratio < eye_det\n",
    "\n",
    "\n",
    "            left_eye_color = (0, 0, 255) if left_eye_det_result else (0, 255, 0)\n",
    "            right_eye_color = (0, 0, 255) if right_eye_det_result else (0, 255, 0)\n",
    "\n",
    "            drowsy_det = left_eye_det_result and right_eye_det_result\n",
    "\n",
    "            if drowsy_det:\n",
    "                det_frame += 1\n",
    "\n",
    "            for i in range(len(eye_x)):\n",
    "                if i <= 5:\n",
    "                    cv2.circle(img0, (eye_x[i], eye_y[i]), 1, left_eye_color, 1)\n",
    "                else:\n",
    "                    cv2.circle(img0, (eye_x[i], eye_y[i]), 1, right_eye_color, 1)\n",
    "                            \n",
    "    writer.write(img0)\n",
    "\n",
    "writer.release()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T10:46:44.412925Z",
     "start_time": "2024-03-17T10:46:38.154222Z"
    }
   },
   "id": "7540f931df2e4153",
   "execution_count": 176
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a5b84a510a106e48"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
